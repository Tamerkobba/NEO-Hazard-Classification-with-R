---
title: ''
output:
  html_document: default
  pdf_document: default
---

# Introduction
In the quest for understanding and safeguarding our planet from potential cosmic threats, the analysis of Near-Earth Objects (NEOs) takes center stage. Leveraging data from NASA's API on NEOs, this report delves into the development of machine learning models aimed at classifying these celestial entities as hazardous or non-hazardous. The urgency of this research lies in the profound implications of a collision with a hazardous NEO, as evidenced by historical cosmic events. By harnessing the power of machine learning, we aspire to enhance our predictive capabilities and contribute to the broader field of planetary defense

# Data Selection
This data is slightly pre-cleaned and obtained through NASA APIs obtained through the Center for Near Earth Object Studies which is responsible for computing highly accurate orbital data for thousands of asteroids and comets that fly close to our planetary neighborhood. This cleaned version is available on Kaggle under public domain provided by Mr. Sameep Vani, and it was last updated in June 2022.

# Project Setup
## Load necessary libraries
```{r}
library(dplyr)
library(lattice)
library(e1071)
library(caTools)
library(caret)
library(pROC)
library(ggplot2)
library(reshape2)
library(FactoMineR)
library(randomForest)
library(cowplot)
library(rpart)
library(rpart.plot)
library(cvms)
```

## Setting the seed
```{r}
set.seed(123)
```

# Data Exploration

## Reading the Dataset
```{r}
raw_data <- read.csv("neo_v2.csv")
raw_data
```

## Size of the Dataset
```{r}
nrow(raw_data)
```
We can see we have 90,836 rows.

```{r}
length(unique(raw_data$id))
```
That said, it's apparent that we only have 27,423 unique objects tracked in this dataset. Meaning certain objects appear more than once as they have orbited the Earth along the years.

## Examining Our Features
Let's now take a look to see what features are present for us in the dataset.
```{r}
columns <- colnames(raw_data)
columns
```

So, we have 10 columns in our data. We have compiled a description of these columns in the table below:

| Column Name        | Data Type | Description                                                                        | Unit                |
|--------------------|-----------|------------------------------------------------------------------------------------|---------------------|
| id                 | int       | Unique identifier for each asteroid                                                | N/A                 |
| name               | str       | Name assigned to the object by NASA                                                | N/A                 |
| est_diameter_min   | float     | Minimum estimate of the diameter of the object                                     | kilometers          |
| est_diameter_max   | float     | Maximum estimate of the diameter of the object                                     | kilometers          |
| relative_velocity  | float     | Velocity of the object relative to Earth                                           | kilometers per hour |
| miss_distance      | float     | Distance missed                                                                    | kilometers          |
| orbiting_body      | str       | Planet that the object orbits                                                      | N/A                 |
| sentry_object      | bool      | Whether the object is included in the sentry-automated collision monitoring system | N/A                 |
| absolute_magnitude | float     | Describes the magnitude of the asteroid based on its intrinsic luminosity          | N/A                 |
| hazardous          | bool      | Whether or not the object is harmful                                               | N/A                 |

## Examining the Distribution of Values
We will now try to visualize the distribution of values in each column of our dataset to get a better understanding of the nature of the data.
```{r}
plot1 <- ggplot(raw_data, aes(x = est_diameter_min)) +
  geom_density() +
  labs(x = "Estimated Diameter (Min)", y = "Density", title = "Density Plot of Minimum Estimated Diameter")
plot2 <- ggplot(raw_data, aes(x = est_diameter_max)) +
  geom_density() +
  labs(x = "Maximum Estimated Diameter", y = "Density", title = "Density Plot of Maximum Estimated Diameter")
plot3 <- ggplot(raw_data, aes(x = relative_velocity)) +
  geom_density() +
  labs(x = "Relative Velocity", y = "Density", title = "Density Plot of Relative Velocity")
plot4 <- ggplot(raw_data, aes(x = miss_distance)) +
  geom_density() +
  labs(x = "Miss Distance", y = "Density", title = "Density Plot of Miss Distance")
plot5 <- ggplot(raw_data, aes(x = absolute_magnitude)) +
  geom_density() +
  labs(x = "Absolute Magnitude", y = "Density", title = "Density Plot of Absolute Magnitude")

plot_grid(plot1, plot2, plot3, plot4, plot5, ncol = 3)
```
```{r}
par(mfrow = c(1, 2))
orbit_cat_count <- table(raw_data$orbiting_body)
pie_chart <- pie(orbit_cat_count, labels = paste(names(orbit_cat_count), "\n", orbit_cat_count), main = "Distribution of Orbiting Bodies")
sentry_cat_count <- table(raw_data$sentry_object)
pie_chart <- pie(sentry_cat_count, labels = paste(names(sentry_cat_count), "\n", sentry_cat_count), main = "Distribution of Sentry Objects")
```

## Under-Sampling
We performed undersampling in such a way to make our dataset balanced between hazardous and non-hazardous objects, while removing non-unique objects since hazard status doesn't change with each observation (hazard status is permanent).

# Model Development

## Decision Trees
The first of a few models we will be trying is Decision Trees.
```{r}
data <- read.csv("balanced_dataset.csv")

feature_names <- c("absolute_magnitude", "miss_distance", "relative_velocity", "est_diameter_min", "est_diameter_max")
features <- data[feature_names]
target <- factor(data$hazardous)

features_scaled <- scale(features)

splitIndex <- createDataPartition(data$hazardous, p = .8, list = FALSE)
X_train <- features_scaled[splitIndex,]
X_test <- features_scaled[-splitIndex,]
y_train <- target[splitIndex]
y_test <- target[-splitIndex]
```

### Fitting the Model

```{r}
model <- rpart(y_train ~ est_diameter_min + est_diameter_max + relative_velocity + miss_distance + absolute_magnitude,
               data = as.data.frame(X_train),
               method = "class",
)
```

Let's examine the model architecture
```{r}
rpart.plot(model)
```

### Model Evaluation
```{r}
predictions <- predict(model, newdata = as.data.frame(X_test), type = "class")

confusion_matrix <- cvms::confusion_matrix(targets = y_test, predictions = predictions)
plot_confusion_matrix(confusion_matrix$`Confusion Matrix`[[1]])
```

We can see our sensitivity (recall) rate is extremely high (97.9%) which is perfect for our use-case since we care more about capturing hazardous objects than we do about correctly detecting harmless objects. On the other hand, our precision is not amazing (79.4%), meaning approximately a quarter of what our model predicted as hazardous was actually benign. The accuracy, balanced accuracy and F1 score metrics can be found below:

```{r}
accuracy <- mean(predictions == y_test)
print(accuracy)
print(confusion_matrix$`Balanced Accuracy`)
print(confusion_matrix$F1)
```

Balanced Accuracy is a better assessment of our model in this case, because balanced accuracy takes into account sensitivity and specificity as well.

### Improvements
We noticed that initially, the tree is too focused on one feature, so we experimented with a few things to increase the model depth, but none of the methods we tried yielded any improvements except for increasing model complexity, with a very slight improvement as can be seen below:

```{r}
complex_model <- rpart(y_train ~ est_diameter_min + est_diameter_max + relative_velocity + miss_distance + absolute_magnitude,
               data = as.data.frame(X_train),
               method = "class",
               control = rpart.control(cp = 0.001),
)
```

Let's also visualize the changes to the tree

```{r}
rpart.plot(complex_model)
```

And let's examine the improvement:
```{r}
complex_predictions <- predict(complex_model, newdata = as.data.frame(X_test), type = "class")

complex_confusion_matrix <- cvms::confusion_matrix(targets = y_test, predictions = complex_predictions)
plot_confusion_matrix(complex_confusion_matrix$`Confusion Matrix`[[1]])
```

We see our model here has dropped a little bit of sensitivity in exchange for slightly higher precision, which raises our accuracy and F1 scores.

```{r}
complex_accuracy <- mean(complex_predictions == y_test)
print(complex_accuracy)
print(complex_confusion_matrix$`Balanced Accuracy`)
print(complex_confusion_matrix$F1)
```

## Ensemble Methods on Trees

```{r}
features <- data[feature_names]
target <- factor(data$hazardous)

features_scaled <- scale(features)

set.seed(123)
splitIndex <- createDataPartition(data$hazardous, p = .8, list = FALSE)
X_train <- features_scaled[splitIndex,]
X_test <- features_scaled[-splitIndex,]
y_train <- target[splitIndex]
y_test <- target[-splitIndex]
```